#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è·¯çº¿1ï¼šGNN+è½¯æ©ç  - è®­ç»ƒè„šæœ¬
ä½¿ç”¨è½¯æ©ç GNNåœ¨å­å›¾ä¸Šè®­ç»ƒï¼ŒåŠ¨æ€å­¦ä¹ è¾¹çš„é‡è¦æ€§
"""

import os
import sys
import json
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import dgl
import matplotlib.pyplot as plt
import seaborn as sns
from tensorboardX import SummaryWriter
from collections import deque
from sklearn.metrics import precision_recall_curve, auc, average_precision_score
from tqdm import tqdm  # å¯¼å…¥tqdmç”¨äºè¿›åº¦æ˜¾ç¤º

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.graph_utils import load_graph_data

# åŠ¨æ€è‡ªé€‚åº”ç¨€ç–æ§åˆ¶å™¨
class AdaptiveSparsityController:
    def __init__(self, target_sparsity=0.25, window_size=10):
        self.target_sparsity = target_sparsity
        self.window_size = window_size
        
        # æ€§èƒ½ç›‘æ§
        self.performance_history = deque(maxlen=window_size)
        self.sparsity_history = deque(maxlen=window_size)
        self.loss_history = deque(maxlen=window_size)
        
        # æœ€ä½³æ€§èƒ½è®°å½•
        self.best_performance = 0.0
        self.total_training_steps = 0
        
        # ä¿®æ”¹ï¼šæ›´ä¿å®ˆçš„æƒé‡è®¾ç½®ï¼Œé˜²æ­¢è¿‡åº¦ç¨€ç–
        self.base_l1_weight = 0.001   # é™ä½åŸºç¡€æƒé‡
        self.max_l1_weight = 0.01     # é™ä½æœ€å¤§æƒé‡
        self.min_l1_weight = 0.0001   # é™ä½æœ€å°æƒé‡
        
        # æ–°å¢ï¼šæ¸©åº¦å‚æ•°æ§åˆ¶
        self.temperature_history = deque(maxlen=5)
        self.target_temperature_range = (0.5, 2.0)  # æ¸©åº¦å®‰å…¨èŒƒå›´
        self.temperature_adjustment_rate = 0.1  # æ¸©åº¦è°ƒæ•´é€Ÿç‡
        
    def update_history(self, performance, sparsity, loss, temperature=None):
        """æ›´æ–°å†å²è®°å½•"""
        self.performance_history.append(performance)
        self.sparsity_history.append(sparsity)
        self.loss_history.append(loss)
        
        if temperature is not None:
            self.temperature_history.append(temperature)
        
        if performance > self.best_performance:
            self.best_performance = performance
    
    def set_total_steps(self, total_steps):
        """è®¾ç½®æ€»è®­ç»ƒæ­¥æ•°"""
        self.total_training_steps = total_steps
    
    def get_training_stage_factor(self, current_step):
        """æ ¹æ®è®­ç»ƒé˜¶æ®µè¿”å›ä¸åŒçš„è°ƒæ•´å› å­ - æ”¹è¿›ç‰ˆ"""
        if self.total_training_steps == 0:
            return 1.0
            
        progress = current_step / self.total_training_steps
        
        if progress < 0.3:
            # å‰30%ï¼šéå¸¸æ¸©å’Œçš„ç¨€ç–åŒ–ï¼Œè®©æ¨¡å‹å…ˆå­¦ä¼šåŸºæœ¬ç‰¹å¾
            return 0.2
        elif progress < 0.6:
            # ä¸­30%ï¼šé€æ¸å¢åŠ ç¨€ç–åŒ–
            return 0.5 + (progress - 0.3) * 1.67  # ä»0.5æ¸å¢åˆ°1.0
        else:
            # å40%ï¼šç¨³å®šçš„ç¨€ç–åŒ–
            return 1.0
    
    def detect_performance_drop(self, threshold=0.03):
        """æ£€æµ‹æ€§èƒ½ä¸‹é™ - é™ä½é˜ˆå€¼ï¼Œæé«˜æ•æ„Ÿåº¦"""
        if len(self.performance_history) < 3:
            return False
        
        recent_avg = np.mean(list(self.performance_history)[-2:])
        earlier_avg = np.mean(list(self.performance_history)[-4:-2])
        
        return (earlier_avg - recent_avg) > threshold
    
    def detect_sparsity_explosion(self, threshold=0.3):
        """æ£€æµ‹ç¨€ç–åº¦çˆ†ç‚¸ - æ–°å¢åŠŸèƒ½"""
        if len(self.sparsity_history) < 2:
            return False
        
        current = self.sparsity_history[-1]
        previous = self.sparsity_history[-2]
        
        # å¦‚æœç¨€ç–åº¦åœ¨ä¸€ä¸ªepochå†…å¢é•¿è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯çˆ†ç‚¸
        return (current - previous) > threshold
    
    def get_sparsity_trend(self):
        """è·å–ç¨€ç–åº¦è¶‹åŠ¿"""
        if len(self.sparsity_history) < 3:
            return 0
        
        recent = list(self.sparsity_history)
        return recent[-1] - recent[0]
    
    def compute_temperature_adjustment(self, current_sparsity, current_temperature):
        """è®¡ç®—æ¸©åº¦å‚æ•°è°ƒæ•´ - æ–°å¢æ ¸å¿ƒåŠŸèƒ½"""
        target_temp = current_temperature
        
        # ç¨€ç–åº¦è¿‡é«˜ï¼šé™ä½æ¸©åº¦ï¼Œä½¿æ©ç æ›´æ¸©å’Œ
        if current_sparsity > self.target_sparsity + 0.15:
            adjustment = -self.temperature_adjustment_rate * min(2.0, current_sparsity / self.target_sparsity)
            target_temp = max(self.target_temperature_range[0], current_temperature + adjustment)
            
        # ç¨€ç–åº¦è¿‡ä½ï¼šé€‚åº¦æé«˜æ¸©åº¦
        elif current_sparsity < self.target_sparsity - 0.1:
            adjustment = self.temperature_adjustment_rate * 0.5  # æ›´ä¿å®ˆçš„å¢é•¿
            target_temp = min(self.target_temperature_range[1], current_temperature + adjustment)
        
        # æ£€æµ‹ç¨€ç–åº¦çˆ†ç‚¸ï¼Œå¼ºåˆ¶é™ä½æ¸©åº¦
        if self.detect_sparsity_explosion():
            target_temp = max(0.3, current_temperature * 0.7)  # å¼ºåˆ¶é™æ¸©
            print(f"âš ï¸ æ£€æµ‹åˆ°ç¨€ç–åº¦çˆ†ç‚¸ï¼Œå¼ºåˆ¶é™æ¸©åˆ° {target_temp:.3f}")
        
        return target_temp
    
    def compute_adaptive_weight(self, current_sparsity, current_performance, training_step):
        """è®¡ç®—è‡ªé€‚åº”ç¨€ç–æƒé‡ - æ”¹è¿›ç‰ˆï¼Œæ›´ä¿å®ˆ"""
        # 1. ç¨€ç–åº¦åå·®é¡¹
        sparsity_deviation = abs(current_sparsity - self.target_sparsity)
        
        # 2. è®­ç»ƒé˜¶æ®µç³»æ•°
        stage_factor = self.get_training_stage_factor(training_step)
        
        # 3. æ›´ä¿å®ˆçš„åŠ¨æ€æƒé‡è®¡ç®—
        if current_sparsity > self.target_sparsity + 0.2:
            # ç¨€ç–åº¦ä¸¥é‡è¿‡é«˜ï¼Œå‡ ä¹åœæ­¢æ­£åˆ™åŒ–
            weight = self.min_l1_weight
        elif current_sparsity > self.target_sparsity + 0.1:
            # ç¨€ç–åº¦è¿‡é«˜ï¼Œå¤§å¹…é™ä½æ­£åˆ™åŒ–
            weight = self.min_l1_weight * 2
        elif current_sparsity < self.target_sparsity - 0.15:
            # ç¨€ç–åº¦è¿‡ä½ï¼Œé€‚åº¦å¢åŠ æ­£åˆ™åŒ–
            weight = self.base_l1_weight + (self.max_l1_weight - self.base_l1_weight) * 0.5
        else:
            # åœ¨åˆç†èŒƒå›´å†…ï¼Œä½¿ç”¨åŸºç¡€æƒé‡
            weight = self.base_l1_weight * stage_factor
        
        # æ€§èƒ½ä¸‹é™ä¿æŠ¤
        if self.detect_performance_drop():
            weight *= 0.5  # æ€§èƒ½ä¸‹é™æ—¶å‡åŠæƒé‡
            print(f"âš ï¸ æ£€æµ‹åˆ°æ€§èƒ½ä¸‹é™ï¼Œé™ä½ç¨€ç–æƒé‡åˆ° {weight:.6f}")
        
        # ç¡®ä¿æƒé‡åœ¨åˆç†èŒƒå›´å†…
        weight = max(self.min_l1_weight, min(self.max_l1_weight, weight))
        
        return weight
    
    def compute_entropy_regularization(self, edge_masks):
        """ç†µæ­£åˆ™åŒ–ï¼šé¼“åŠ±æ©ç åˆ†å¸ƒçš„å¤šæ ·æ€§ - æ”¹è¿›ç‰ˆ"""
        if len(edge_masks) == 0:
            return torch.tensor(0.0, device=edge_masks.device if hasattr(edge_masks, 'device') else 'cpu')
        
        # é¿å…æ‰€æœ‰æ©ç éƒ½è¶‹å‘åŒä¸€ä¸ªå€¼
        p = torch.clamp(edge_masks, 1e-8, 1-1e-8)
        entropy = -(p * torch.log(p) + (1-p) * torch.log(1-p)).mean()
        
        # æ ¹æ®å½“å‰ç¨€ç–åº¦è°ƒæ•´ç†µæƒé‡
        current_sparsity = (edge_masks < 0.5).float().mean()
        if current_sparsity > 0.8:
            # è¿‡åº¦ç¨€ç–æ—¶ï¼Œå¼ºåŒ–ç†µæ­£åˆ™åŒ–ï¼Œé¼“åŠ±å¤šæ ·æ€§
            return -entropy * 2.0
        else:
            return -entropy * 0.5  # æ­£å¸¸æƒ…å†µä¸‹ä½¿ç”¨è¾ƒå°æƒé‡

# è½¯æ©ç GNNæ¨¡å‹
class SoftMaskGNN(nn.Module):
    def __init__(self, in_dim, hidden_dim, num_layers=2, dropout=0.2, sparsity_target=0.25):
        super(SoftMaskGNN, self).__init__()
        self.in_dim = in_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout
        self.sparsity_target = sparsity_target
        
        # åˆå§‹åŒ–è‡ªé€‚åº”ç¨€ç–æ§åˆ¶å™¨
        self.sparsity_controller = AdaptiveSparsityController(target_sparsity=sparsity_target)
        
        # èŠ‚ç‚¹ç±»å‹ç‰¹å®šçš„å˜æ¢
        self.question_transform = nn.Linear(in_dim, hidden_dim)
        self.entity_transform = nn.Linear(in_dim, hidden_dim)
        self.context_transform = nn.Linear(in_dim, hidden_dim)
        # æ·»åŠ ï¼šè®­ç»ƒæ­¥æ•°è¿½è¸ª
        self.register_buffer('training_steps', torch.tensor(0.0))
        
        # ä¿®æ”¹ï¼šæ›´ä¿å®ˆçš„åˆå§‹æ¸©åº¦å’Œå‚æ•°è®¾ç½®
        self.mask_temperature = nn.Parameter(torch.tensor(0.8))  # é™ä½åˆå§‹æ¸©åº¦
        
        # æ–°å¢ï¼šæ¸©åº¦æ§åˆ¶ç›¸å…³å‚æ•°
        self.temperature_bounds = (0.3, 2.5)  # æ¸©åº¦è¾¹ç•Œ
        self.register_buffer('last_sparsity', torch.tensor(0.0))  # è®°å½•ä¸Šæ¬¡ç¨€ç–åº¦
        
        # è¾¹ç±»å‹ç‰¹å®šçš„å˜æ¢
        self.edge_transforms = nn.ModuleDict({
            'answers': nn.Linear(hidden_dim, hidden_dim),
            'evidencedBy': nn.Linear(hidden_dim, hidden_dim),
            'supportsAnswer': nn.Linear(hidden_dim, hidden_dim),
            'relatedTo': nn.Linear(hidden_dim, hidden_dim),
            'default': nn.Linear(hidden_dim, hidden_dim)
        })
        
        # è¾¹é‡è¦æ€§é¢„æµ‹ç½‘ç»œ (ä½¿ç”¨Sigmoidç¡®ä¿å€¼åœ¨0-1ä¹‹é—´)
        self.edge_importance = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # è¾¹æ©ç æ¸©åº¦å‚æ•°ï¼ˆå¯è®­ç»ƒï¼‰
        self.mask_temperature = nn.Parameter(torch.tensor(1.0))
        
        # æ¶ˆæ¯ä¼ é€’å±‚
        self.layers = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.output = nn.Linear(hidden_dim, 1)
        
        self.dropout_layer = nn.Dropout(dropout)
    
    def forward(self, g, node_feats, edge_weights=None):
        """
        å‰å‘ä¼ æ’­æ¥è·å–èŠ‚ç‚¹è¡¨ç¤ºå’Œè¾¹æ©ç 
        
        å‚æ•°:
        - g: DGLå›¾
        - node_feats: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, in_dim]
        - edge_weights: å¯é€‰çš„é¢„å®šä¹‰è¾¹æƒé‡
        
        è¿”å›:
        - h: èŠ‚ç‚¹åµŒå…¥ [num_nodes, hidden_dim]
        - edge_masks: è¾¹æ©ç  [num_edges, 1]
        - sparsity: æ©ç ç¨€ç–ç‡
        """
        # åˆå§‹åŒ–èŠ‚ç‚¹åµŒå…¥
        h = torch.zeros(node_feats.size(0), self.hidden_dim, device=node_feats.device)
        
        # æ ¹æ®èŠ‚ç‚¹è§’è‰²åº”ç”¨ä¸åŒçš„å˜æ¢
        if hasattr(g, 'ndata') and 'role' in g.ndata:
            roles = g.ndata['role']
            # ç¡®ä¿è§’è‰²æ•°æ®æ˜¯åˆé€‚çš„å°ºå¯¸
            if len(roles) != node_feats.size(0):
                # å¦‚æœè§’è‰²å’ŒèŠ‚ç‚¹ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œä¸ºæ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨é»˜è®¤å˜æ¢
                h = self.context_transform(node_feats)
            else:
                for i in range(node_feats.size(0)):
                    role = roles[i].item() if isinstance(roles[i], torch.Tensor) else roles[i]
                    
                    # åŸºäºè§’è‰²é€‰æ‹©å˜æ¢
                    if role == 0:  # é—®é¢˜èŠ‚ç‚¹
                        h[i] = self.question_transform(node_feats[i])
                    elif role in [2, 3]:  # ç­”æ¡ˆ/è¯æ®èŠ‚ç‚¹
                        h[i] = self.entity_transform(node_feats[i])
                    else:  # é»˜è®¤ä¸ºä¸Šä¸‹æ–‡èŠ‚ç‚¹
                        h[i] = self.context_transform(node_feats[i])
        else:
            # å¦‚æœæ²¡æœ‰è§’è‰²ä¿¡æ¯ï¼Œå¯¹æ‰€æœ‰èŠ‚ç‚¹åº”ç”¨ç›¸åŒçš„å˜æ¢
            h = self.context_transform(node_feats)
        
        # åˆå§‹åŒ–ç‰¹å¾
        h = F.relu(h)
        
        # è·å–è¾¹çš„æºå’Œç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
        edge_src, edge_dst = g.edges()
        num_edges = len(edge_src)
        
        # åˆ›å»ºè¾¹ç‰¹å¾ï¼Œæ‹¼æ¥è¾¹çš„æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹è¡¨ç¤º
        edge_feats = []
        for i in range(num_edges):
            src, dst = edge_src[i], edge_dst[i]
            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            if src < h.size(0) and dst < h.size(0):
                edge_feat = torch.cat([h[src], h[dst]], dim=0)
                edge_feats.append(edge_feat)
        
        if len(edge_feats) > 0:
            edge_feats = torch.stack(edge_feats)
            # å­¦ä¹ è¾¹æ©ç ï¼ˆæ¯æ¡è¾¹ä¸€ä¸ªæ©ç å€¼ï¼‰
            edge_importances = self.edge_importance(edge_feats).squeeze()
            
            # ä¿®æ”¹ï¼šä½¿ç”¨æ›´åˆç†çš„é˜ˆå€¼å’Œæ¸©åº¦
            edge_masks = torch.sigmoid((edge_importances - 0.5) * torch.exp(self.mask_temperature))
            
            # ä¿®æ”¹ï¼šæ·»åŠ "ç»“æ„åŒ–å™ªå£°"ä»¥æé«˜æ©ç å¤šæ ·æ€§
            if self.training:
                # åœ¨è®­ç»ƒæœŸé—´æ·»åŠ ç»“æ„åŒ–å™ªå£°
                noise = torch.randn_like(edge_importances) * 0.02  # è¿›ä¸€æ­¥é™ä½å™ªå£°å¼ºåº¦
                
                # å®‰å…¨è·å–is_gold_path
                is_gold_path = None
                if hasattr(g, 'edata'):
                    if 'is_gold_path' in g.edata:
                        is_gold_path = g.edata['is_gold_path']
                        # ç¡®ä¿å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                        if is_gold_path.device != edge_importances.device:
                            is_gold_path = is_gold_path.to(edge_importances.device)
                            
                        # ç¡®ä¿å°ºå¯¸åŒ¹é…
                        if len(is_gold_path) == len(edge_importances):
                            # ä½¿ç”¨å¸ƒå°”æ©ç è¿›è¡Œç´¢å¼•
                            gold_edges = (is_gold_path > 0)
                            if gold_edges.any():
                                noise[gold_edges] *= 0.1  # å¤§å¹…å‡å°‘é»„é‡‘è·¯å¾„è¾¹çš„å™ªå£°
                edge_masks = torch.sigmoid((edge_importances + noise - 0.5) * torch.exp(self.mask_temperature))
            
            # å¦‚æœæä¾›äº†å…ˆéªŒæƒé‡ï¼Œåˆ™ä¸å­¦ä¹ åˆ°çš„é‡è¦æ€§ç›¸ç»“åˆ
            if edge_weights is not None and len(edge_weights) > 0:
                edge_weights = edge_weights.view(-1)
                if len(edge_weights) != len(edge_masks):
                    # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨å¹¿æ’­æˆ–è€…æ‰©å±•
                    if len(edge_weights) == 1:
                        # å•ä¸€æƒé‡ï¼Œç›´æ¥å¹¿æ’­
                        edge_masks = edge_masks * edge_weights
                    else:
                        # å°ºå¯¸ä¸åŒ¹é…ï¼Œå–æœ€å°é•¿åº¦
                        min_len = min(len(edge_masks), len(edge_weights))
                        edge_masks = edge_masks[:min_len] * edge_weights[:min_len]
                else:
                    edge_masks = edge_masks * edge_weights
        else:
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œåˆ›å»ºç©ºæ©ç 
            edge_masks = torch.tensor([], device=node_feats.device)
        
        # å¯¹æ¯ä¸€å±‚è¿›è¡Œæ¶ˆæ¯ä¼ é€’
        for layer_idx, layer in enumerate(self.layers):
            # åˆ›å»ºæ–°çš„èŠ‚ç‚¹è¡¨ç¤º
            new_h = torch.zeros_like(h)
            
            # å¯¹æ¯æ¡è¾¹è¿›è¡Œæ¶ˆæ¯ä¼ é€’
            for i in range(num_edges):
                src, dst = edge_src[i], edge_dst[i]
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                if src >= h.size(0) or dst >= h.size(0):
                    continue
                    
                # è·å–è¾¹ç±»å‹
                edge_type = 'default'
                if hasattr(g, 'edata') and 'rel' in g.edata and i < len(g.edata['rel']):
                    edge_type = g.edata['rel'][i].item() if isinstance(g.edata['rel'][i], torch.Tensor) else g.edata['rel'][i]
                
                # è·å–å¯¹åº”çš„è¾¹å˜æ¢
                if edge_type in self.edge_transforms:
                    transform = self.edge_transforms[edge_type]
                else:
                    transform = self.edge_transforms['default']
                
                # è·å–è¯¥è¾¹çš„æ©ç 
                mask_val = edge_masks[i] if i < len(edge_masks) else torch.tensor(1.0, device=h.device)
                
                # å…ˆåº”ç”¨è½¬æ¢ï¼Œå†åº”ç”¨æ©ç 
                src_transformed = transform(h[src])
                if isinstance(mask_val, torch.Tensor) and mask_val.dim() > 0:
                    if mask_val.size(0) != 1:
                        mask_val = mask_val.mean()  # è½¬ä¸ºæ ‡é‡
                
                # æ©ç åçš„æ¶ˆæ¯
                message = src_transformed * mask_val
                
                # ç´¯ç§¯æ¶ˆæ¯åˆ°ç›®æ ‡èŠ‚ç‚¹
                new_h[dst] += message
            
            # æ›´æ–°èŠ‚ç‚¹è¡¨ç¤ºï¼ˆåŒ…æ‹¬æ®‹å·®è¿æ¥ï¼‰
            h = F.relu(layer(new_h + h))
            h = self.dropout_layer(h)
        
        # ä¿®æ”¹ï¼šä½¿ç”¨æ›´åˆç†çš„ç¨€ç–åº¦é˜ˆå€¼
        sparsity = (edge_masks < 0.5).float().mean().item() if len(edge_masks) > 0 else 0.0
        
        # è¿”å›èŠ‚ç‚¹è¡¨ç¤ºã€è¾¹æ©ç å’Œç¨€ç–ç‡
        return h, edge_masks, sparsity
    
    def compute_answer_scores(self, g, node_feats, edge_weights, question_idx, candidate_idxs):
        """è®¡ç®—æ‰€æœ‰å€™é€‰ç­”æ¡ˆçš„åˆ†æ•°"""
        # è·å–æ‰€æœ‰èŠ‚ç‚¹çš„è¡¨ç¤ºå’Œè¾¹é‡è¦æ€§
        node_embeddings, edge_masks, sparsity = self.forward(g, node_feats, edge_weights)
        
        # è·å–é—®é¢˜èŠ‚ç‚¹çš„è¡¨ç¤º
        q_embedding = node_embeddings[question_idx]
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰ç­”æ¡ˆçš„åˆ†æ•°
        scores = torch.zeros(len(candidate_idxs), device=node_feats.device)
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        q_embed_reshaped = q_embedding.unsqueeze(0)  # [1, hidden_dim]
        
        for i, ans_idx in enumerate(candidate_idxs):
            # å®‰å…¨æ£€æŸ¥é¿å…ç´¢å¼•è¶Šç•Œ
            if ans_idx < node_embeddings.size(0):
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                ans_embedding = node_embeddings[ans_idx].unsqueeze(0)  # [1, hidden_dim]
                scores[i] = F.cosine_similarity(q_embed_reshaped, ans_embedding, dim=1)[0]
            else:
                # å¯¹äºæ— æ•ˆç´¢å¼•ï¼Œè®¾ç½®ä½åˆ†
                scores[i] = torch.tensor(-1.0, device=scores.device)
        
        return scores, edge_masks, sparsity
    
    def compute_adaptive_sparsity_loss(self, edge_masks, current_performance, training_step):
        """è®¡ç®—è‡ªé€‚åº”ç¨€ç–æ€§æŸå¤±"""
        if len(edge_masks) == 0:
            return torch.tensor(0.0, device=edge_masks.device if hasattr(edge_masks, 'device') else 'cpu'), 0.0
        
        # å½“å‰ç¨€ç–åº¦
        current_sparsity = (edge_masks < 0.5).float().mean()
        
        # åŠ¨æ€æƒé‡
        adaptive_weight = self.sparsity_controller.compute_adaptive_weight(
            current_sparsity.item(), current_performance, training_step
        )
        
        # åŸºç¡€ç¨€ç–æŸå¤±
        sparsity_distance = F.mse_loss(
            current_sparsity, 
            torch.tensor(self.sparsity_target, device=edge_masks.device)
        )
        
        # å¤šå±‚æ¬¡æ­£åˆ™åŒ–
        l1_reg = edge_masks.mean()  # ä¿®æ”¹ï¼šä½¿ç”¨meanè€Œä¸æ˜¯abs().mean()
        entropy_reg = self.sparsity_controller.compute_entropy_regularization(edge_masks)
        
        # ç»„åˆæŸå¤±
        total_sparsity_loss = (
            sparsity_distance + 
            adaptive_weight * l1_reg + 
            0.005 * entropy_reg  # é™ä½ç†µæ­£åˆ™åŒ–æƒé‡
        )
        
        return total_sparsity_loss, adaptive_weight

    def compute_node_embeddings(self, g, node_feats, edge_weights=None):
        """è®¡ç®—èŠ‚ç‚¹åµŒå…¥ï¼Œä¸è®¡ç®—é¢„æµ‹åˆ†æ•°"""
        # è°ƒç”¨forwardä½†åªè¿”å›èŠ‚ç‚¹åµŒå…¥
        h, _, _ = self.forward(g, node_feats, edge_weights)
        return h

# å›¾æ•°æ®é›†
class GraphDataset(Dataset):
    def __init__(self, graph_dir, graph_files=None, max_nodes=100):
        self.graph_dir = graph_dir
        self.max_nodes = max_nodes
        
        # åŠ è½½å›¾æ–‡ä»¶åˆ—è¡¨
        if graph_files is None:
            self.graph_files = [f for f in os.listdir(graph_dir) if f.endswith('.json')]
        else:
            self.graph_files = graph_files
        
        print(f"åŠ è½½äº† {len(self.graph_files)} ä¸ªå›¾æ–‡ä»¶")
    
    def __len__(self):
        return len(self.graph_files)
    
    def __getitem__(self, idx):
        graph_file = os.path.join(self.graph_dir, self.graph_files[idx])
        
        # åŠ è½½å›¾æ•°æ®
        with open(graph_file, 'r', encoding='utf-8') as f:
            graph_data = json.load(f)
        
        # æ„å»ºDGLå›¾
        g, node_feats, edge_weights, meta_info = self.build_graph(graph_data)
        
        return {
            'graph': g,
            'node_feats': node_feats,
            'edge_weights': edge_weights,
            'question_idx': meta_info['question_idx'],
            'answer_idx': meta_info['answer_idx'],
            'candidate_idxs': meta_info['candidate_idxs'],
            'graph_id': graph_data.get('id', self.graph_files[idx])
        }
    
    def build_graph(self, graph_data):
        """å°†JSONå›¾è½¬æ¢ä¸ºDGLå›¾"""
        nodes = graph_data.get('nodes', [])
        edges = graph_data.get('edges', [])
        
        # é™åˆ¶èŠ‚ç‚¹æ•°é‡
        if len(nodes) > self.max_nodes:
            nodes = nodes[:self.max_nodes]
        
        # åˆ›å»ºèŠ‚ç‚¹ç‰¹å¾
        node_feats = []
        node_roles = []
        
        # è®°å½•é—®é¢˜å’Œç­”æ¡ˆèŠ‚ç‚¹çš„ç´¢å¼•
        question_idx = -1
        answer_idx = -1
        candidate_idxs = []
        
        # å¤„ç†èŠ‚ç‚¹
        for i, node in enumerate(nodes):
            # èŠ‚ç‚¹ç‰¹å¾
            if 'feat' in node and node['feat'] != 'PLACEHOLDER':
                try:
                    feat = torch.tensor(node['feat'], dtype=torch.float)
                    if feat.shape[0] < 768:
                        feat = torch.cat([feat, torch.zeros(768-feat.shape[0])])
                    elif feat.shape[0] > 768:
                        feat = feat[:768]
                except:
                    feat = torch.randn(768)
            else:
                feat = torch.randn(768)
            
            node_feats.append(feat)
            
            # èŠ‚ç‚¹è§’è‰²
            role = node.get('role', 'context')
            node_roles.append(role)
            
            # è®°å½•ç‰¹æ®ŠèŠ‚ç‚¹
            if role == 'question':
                question_idx = i
            elif role == 'answer':
                answer_idx = i
                candidate_idxs.append(i)
            elif role == 'evidence':
                # è¯æ®èŠ‚ç‚¹ä¹Ÿå¯èƒ½æ˜¯å€™é€‰ç­”æ¡ˆ
                candidate_idxs.append(i)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é—®é¢˜èŠ‚ç‚¹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
        if question_idx == -1 and len(nodes) > 0:
            question_idx = 0
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆèŠ‚ç‚¹ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªèŠ‚ç‚¹
        if answer_idx == -1 and len(nodes) > 0:
            answer_idx = len(nodes) - 1
            candidate_idxs.append(answer_idx)
        
        # å¦‚æœæ²¡æœ‰å€™é€‰ç­”æ¡ˆï¼Œä½¿ç”¨æ‰€æœ‰éé—®é¢˜èŠ‚ç‚¹
        if not candidate_idxs:
            candidate_idxs = [i for i in range(len(nodes)) if i != question_idx]
        
        # è½¬æ¢ä¸ºå¼ é‡
        node_feats = torch.stack(node_feats)
        
        # å¤„ç†è¾¹
        src_ids = []
        dst_ids = []
        edge_types = []
        edge_weights = []
        
        for edge in edges:
            src = edge.get('src', '').replace('n', '')
            dst = edge.get('dst', '').replace('n', '')
            
            # æ£€æŸ¥è¾¹çš„æœ‰æ•ˆæ€§
            if not src.isdigit() or not dst.isdigit():
                continue
            
            src_id, dst_id = int(src), int(dst)
            
            # ç¡®ä¿èŠ‚ç‚¹ç´¢å¼•æœ‰æ•ˆ
            if src_id >= len(nodes) or dst_id >= len(nodes):
                continue
            
            # æ·»åŠ è¾¹
            src_ids.append(src_id)
            dst_ids.append(dst_id)
            
            # è¾¹ç±»å‹
            rel = edge.get('rel', 'default')
            edge_types.append(rel)
            
            # è¾¹æƒé‡
            weight = edge.get('weight', 1.0)
            edge_weights.append(weight)
        
        # åˆ›å»ºDGLå›¾
        g = dgl.graph((src_ids, dst_ids), num_nodes=len(nodes))
        
        # ç¡®ä¿tensoré•¿åº¦ä¸å›¾ä¸­èŠ‚ç‚¹æ•°é‡åŒ¹é…
        role_map = {'question': 0, 'context': 1, 'answer': 2, 'evidence': 3, 'distractor': 4}
        numeric_roles = [role_map.get(role, 0) for role in node_roles]
        g.ndata['role'] = torch.tensor(numeric_roles[:g.number_of_nodes()])
        
        # è¾¹ç±»å‹è½¬ä¸ºæ•°å­—ID
        if edge_types:
            edge_type_set = list(sorted(set(edge_types)))
            edge_type_map = {etype: idx for idx, etype in enumerate(edge_type_set)}
            numeric_edge_types = [edge_type_map[etype] for etype in edge_types]
            g.edata['rel'] = torch.tensor(numeric_edge_types, dtype=torch.long)
        
        if edge_weights:
            edge_weights = torch.tensor(edge_weights, dtype=torch.float)
        else:
            edge_weights = torch.ones(g.number_of_edges(), dtype=torch.float)
        
        # å…ƒä¿¡æ¯
        meta_info = {
            'question_idx': question_idx,
            'answer_idx': answer_idx,
            'candidate_idxs': candidate_idxs
        }
        
        return g, node_feats, edge_weights, meta_info

def collate_fn(samples):
    graphs = [s['graph'] for s in samples]
    
    # æ£€æŸ¥å›¾æ˜¯å¦ä¸ºå¼‚æ„å›¾
    is_hetero = False
    if graphs and hasattr(graphs[0], 'ntypes') and len(graphs[0].ntypes) > 1:
        is_hetero = True
    
    if is_hetero:
        # å¯¹äºå¼‚æ„å›¾ï¼Œä¸ä½¿ç”¨batchæ“ä½œï¼Œç›´æ¥ä¿å­˜å›¾åˆ—è¡¨
        batched_graph = graphs
    else:
        # å¯¹äºåŒæ„å›¾ï¼Œä½¿ç”¨dgl.batch
        batched_graph = dgl.batch(graphs)
    
    node_feats = torch.cat([s['node_feats'] for s in samples], dim=0)
    edge_weights = torch.cat([s['edge_weights'] for s in samples], dim=0)
    question_idx = torch.tensor([s['question_idx'] for s in samples], dtype=torch.long)
    answer_idx = torch.tensor([s['answer_idx'] for s in samples], dtype=torch.long)
    candidate_idxs = [s['candidate_idxs'] for s in samples]
    graph_id = [s['graph_id'] for s in samples]
    return {
        'graph': batched_graph,
        'node_feats': node_feats,
        'edge_weights': edge_weights,
        'question_idx': question_idx,
        'answer_idx': answer_idx,
        'candidate_idxs': candidate_idxs,
        'graph_id': graph_id
    }

def contrastive_loss(node_embeds, question_idx, answer_idxs, negative_idxs, temperature=0.1):
    """æ·»åŠ å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œæ‹‰è¿‘é—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆï¼Œæ¨è¿œé—®é¢˜å’Œé”™è¯¯ç­”æ¡ˆ"""
    # ç¡®ä¿é—®é¢˜ç´¢å¼•æ˜¯å•ä¸ªå€¼ï¼Œå¦‚æœæ˜¯æ‰¹æ¬¡åˆ™å–ç¬¬ä¸€ä¸ª
    if isinstance(question_idx, torch.Tensor) and question_idx.dim() > 0:
        q_idx = question_idx[0].item()
    else:
        q_idx = question_idx
        
    # è·å–é—®é¢˜åµŒå…¥
    q_embed = node_embeds[q_idx]
    
    # ç¡®ä¿ç­”æ¡ˆç´¢å¼•æ˜¯ä¸€ç»´å¼ é‡
    if isinstance(answer_idxs, torch.Tensor) and answer_idxs.dim() > 1:
        ans_idxs = answer_idxs.flatten()
    else:
        ans_idxs = answer_idxs
    
    # æ­£ä¾‹ï¼šæ­£ç¡®ç­”æ¡ˆ
    pos_embeds = node_embeds[ans_idxs]
    
    # ç¡®ä¿å½¢çŠ¶æ­£ç¡®ï¼Œä¿æŒç»´åº¦ä¸€è‡´æ€§
    q_embed_reshaped = q_embed.view(1, -1)  # å°†é—®é¢˜åµŒå…¥é‡å¡‘ä¸º [1, dim]
    pos_sim = F.cosine_similarity(q_embed_reshaped, pos_embeds, dim=1) / temperature
    
    # è´Ÿä¾‹ï¼šé”™è¯¯ç­”æ¡ˆæˆ–éšæœºèŠ‚ç‚¹
    neg_embeds = node_embeds[negative_idxs]
    
    # ç¡®ä¿é—®é¢˜åµŒå…¥ä¸è´Ÿä¾‹åµŒå…¥çš„ç»´åº¦åŒ¹é…
    # é‡å¡‘é—®é¢˜åµŒå…¥ä»¥åŒ¹é…è´Ÿä¾‹çš„ç¬¬äºŒç»´åº¦
    if neg_embeds.size(1) != q_embed.size(0):
        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è´Ÿä¾‹åµŒå…¥
        neg_embeds = neg_embeds[:, :q_embed.size(0)]
    
    neg_sim = F.cosine_similarity(q_embed_reshaped, neg_embeds, dim=1) / temperature
    
    # æ­£è§„åŒ–ç›¸ä¼¼åº¦
    logits = torch.cat([pos_sim, neg_sim])
    labels = torch.zeros(len(logits), device=node_embeds.device)
    labels[:len(pos_sim)] = 1.0
    
    # è®¡ç®—å¯¹æ¯”æŸå¤±
    loss = F.binary_cross_entropy_with_logits(logits, labels)
    return loss

def train(model, train_loader, optimizer, device, epoch, writer, sparsity_weight=0.01):
    """è®­ç»ƒä¸€ä¸ªepochå¹¶è¿”å›æŒ‡æ ‡"""
    model.train()
    total_loss = 0
    total_sparsity_loss = 0
    total_task_loss = 0
    correct = 0
    total = 0
    
    # è·Ÿè¸ªæ‰€æœ‰å›¾çš„æ©ç ç¨€ç–ç‡
    all_sparsities = []
    # æ·»åŠ F1åˆ†æ•°è·Ÿè¸ª
    all_f1_scores = []
    # è·Ÿè¸ªè‡ªé€‚åº”æƒé‡
    all_adaptive_weights = []
    
    # é¢„è®­ç»ƒé˜¶æ®µï¼ˆå‰å‡ ä¸ªepochï¼‰
    if epoch < 3:
        print(f"æ‰§è¡Œé¢„è®­ç»ƒ (Epoch {epoch+1}/3)...")
        
        epoch_loss = 0
        epoch_correct = 0
        epoch_total = 0
        
        for batch_idx, batch in enumerate(train_loader):
            # è·å–æ‰¹æ¬¡æ•°æ®å¹¶ç§»è‡³è®¾å¤‡
            g = batch['graph'].to(device)
            node_feats = batch['node_feats'].to(device)
            edge_weights = batch['edge_weights'].to(device)
            question_idx = batch['question_idx'].to(device)
            answer_idx = batch['answer_idx'].to(device)
            candidate_idxs = batch['candidate_idxs']
            
            # æ¸…é™¤æ¢¯åº¦
            optimizer.zero_grad()
            
            # ä¿®æ”¹ï¼šä½¿ç”¨æ­£å¸¸çš„å‰å‘ä¼ æ’­è€Œä¸æ˜¯åªè·å–è¾¹æ©ç 
            batch_scores = []
            batch_labels = []
            batch_masks = []
            
            # è·å–æ‰¹é‡å¤§å°
            batch_size = g.batch_size if hasattr(g, 'batch_size') else len(candidate_idxs)
            is_hetero = hasattr(g, 'ntypes') and len(g.ntypes) > 1
            
            for i in range(batch_size):
                candidates = candidate_idxs[i]
                
                if is_hetero:
                    current_g = g[i] if isinstance(g, list) else g
                    current_feats = node_feats[i]
                    current_weights = edge_weights[i] if edge_weights.dim() > 1 else edge_weights
                    current_q_idx = question_idx[i]
                else:
                    current_g = g
                    current_feats = node_feats
                    current_weights = edge_weights
                    current_q_idx = question_idx[i]
                
                scores, edge_masks, _ = model.compute_answer_scores(
                    current_g, current_feats, current_weights, 
                    current_q_idx, candidates
                )
                
                # åˆ›å»ºæ ‡ç­¾
                labels = torch.zeros_like(scores)
                for j, c_idx in enumerate(candidates):
                    if c_idx == answer_idx[i].item():
                        labels[j] = 1
                        break
                
                batch_scores.append(scores)
                batch_labels.append(labels)
                batch_masks.append(edge_masks)
            
            # è®¡ç®—ä¸»ä»»åŠ¡æŸå¤±
            scores = torch.cat(batch_scores)
            labels = torch.cat(batch_labels)
            
            # é¢„è®­ç»ƒä½¿ç”¨è¾ƒå°çš„ç¨€ç–æ­£åˆ™åŒ–
            task_loss = F.binary_cross_entropy_with_logits(scores, labels)
            
            # è½»å¾®çš„ç¨€ç–æ­£åˆ™åŒ–ï¼ˆé¢„è®­ç»ƒé˜¶æ®µï¼‰
            all_masks = torch.cat(batch_masks)
            sparsity_loss = 0.001 * all_masks.mean()  # å¾ˆå°çš„L1æ­£åˆ™åŒ–
            
            pretrain_loss = task_loss + sparsity_loss
            pretrain_loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            epoch_loss += pretrain_loss.item()
            pred = (scores > 0.5).float()
            current_correct = (pred == labels).sum().item()
            epoch_correct += current_correct
            epoch_total += len(scores)
            
            # æ›´æ–°è®­ç»ƒæ­¥æ•°
            with torch.no_grad():
                model.training_steps += 1.0
                
            # è®°å½•åˆ°TensorBoard
            global_step = epoch * len(train_loader) + batch_idx
            writer.add_scalar('Pretrain/Loss', pretrain_loss.item(), global_step)
            writer.add_scalar('Pretrain/TaskLoss', task_loss.item(), global_step)
            writer.add_scalar('Pretrain/SparsityLoss', sparsity_loss.item(), global_step)
        
        # è®¡ç®—é¢„è®­ç»ƒepochå¹³å‡å€¼
        avg_pretrain_loss = epoch_loss / len(train_loader)
        pretrain_accuracy = epoch_correct / epoch_total if epoch_total > 0 else 0
        
        print(f"é¢„è®­ç»ƒ Epoch {epoch}: Loss: {avg_pretrain_loss:.4f}, Acc: {pretrain_accuracy:.4f}")
        
        # è¿”å›é¢„è®­ç»ƒç»“æœ
        return avg_pretrain_loss, pretrain_accuracy, 0.0, 0.0
    
    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),
                        desc=f"Epoch {epoch+1}", leave=True)
    
    for batch_idx, batch in progress_bar:
        g = batch['graph'].to(device)
        node_feats = batch['node_feats'].to(device)
        edge_weights = batch['edge_weights'].to(device)
        question_idx = batch['question_idx'].to(device)
        answer_idx = batch['answer_idx'].to(device)
        candidate_idxs = batch['candidate_idxs']
        
        # æ¸…é™¤æ¢¯åº¦
        optimizer.zero_grad()
        
        # åœ¨è®¡ç®—è´Ÿä¾‹ç´¢å¼•ä¹‹å‰ï¼Œé¦–å…ˆè·å–node_embeds
        node_embeds = None
        # å°è¯•ä»æ¨¡å‹è¾“å‡ºè·å–èŠ‚ç‚¹åµŒå…¥
        try:
            # å¯¹æ•´ä¸ªæ‰¹æ¬¡å›¾æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè·å–æ‰€æœ‰èŠ‚ç‚¹åµŒå…¥
            node_embeds, _, _ = model(g, node_feats, edge_weights)
        except Exception as e:
            print(f"è·å–èŠ‚ç‚¹åµŒå…¥æ—¶å‡ºé”™: {e}")
            # å¦‚æœæ‰¹å¤„ç†å‰å‘ä¼ æ’­å¤±è´¥ï¼Œå°è¯•å¯¹ç¬¬ä¸€ä¸ªå›¾å•ç‹¬å¤„ç†
            if batch_size > 0:
                try:
                    if is_hetero:
                        current_g = g[0] if isinstance(g, list) else g
                        current_feats = node_feats[0] if node_feats.dim() > 1 else node_feats
                        current_weights = edge_weights[0] if edge_weights.dim() > 1 else edge_weights
                    else:
                        # å¯¹äºåŒæ„å›¾ï¼Œä½¿ç”¨å®Œæ•´å›¾ä½†åªæå–ç¬¬ä¸€ä¸ªå›¾çš„åµŒå…¥
                        current_g = g
                        current_feats = node_feats
                        current_weights = edge_weights
                    
                    node_embeds, _, _ = model(current_g, current_feats, current_weights)
                except Exception as e:
                    print(f"å°è¯•å¤„ç†ç¬¬ä¸€ä¸ªå›¾ä¹Ÿå¤±è´¥: {e}")
                    node_embeds = None
        
        # è®¡ç®—å€™é€‰ç­”æ¡ˆçš„åˆ†æ•°
        batch_scores = []
        batch_labels = []
        batch_masks = []
        batch_sparsities = []
        
        # è·å–æ‰¹é‡å¤§å° - é¿å…ä½¿ç”¨len(g)
        batch_size = g.batch_size if hasattr(g, 'batch_size') else len(candidate_idxs)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚æ„å›¾
        is_hetero = hasattr(g, 'ntypes') and len(g.ntypes) > 1
        
        for i in range(batch_size):
            # è·å–å½“å‰å›¾çš„å€™é€‰ç­”æ¡ˆç´¢å¼•
            candidates = candidate_idxs[i]
            
            # è®¡ç®—æ¯ä¸ªå€™é€‰ç­”æ¡ˆçš„åˆ†æ•°å¹¶è·å–è¾¹æ©ç 
            if is_hetero:
                # å¯¹äºå¼‚æ„å›¾ï¼Œæˆ‘ä»¬éœ€è¦å•ç‹¬å¤„ç†æ¯ä¸ªå›¾
                current_g = g[i] if isinstance(g, list) else g
                current_feats = node_feats[i]
                current_weights = edge_weights[i] if edge_weights.dim() > 1 else edge_weights
                current_q_idx = question_idx[i]
            else:
                # å¯¹äºåŒæ„å›¾ï¼Œä½¿ç”¨æ‰¹å¤„ç†ç´¢å¼•
                current_g = g
                current_feats = node_feats
                current_weights = edge_weights
                current_q_idx = question_idx[i]
            
            scores, edge_masks, sparsity = model.compute_answer_scores(
                current_g, current_feats, current_weights, 
                current_q_idx, candidates
            )
            
            # åˆ›å»ºæ ‡ç­¾ï¼Œæ­£ç¡®çš„ç­”æ¡ˆæ ‡è®°ä¸º1ï¼Œå…¶ä»–æ ‡è®°ä¸º0
            labels = torch.zeros_like(scores)
            for j, c_idx in enumerate(candidates):
                if c_idx == answer_idx[i].item():
                    labels[j] = 1
                    break
            
            batch_scores.append(scores)
            batch_labels.append(labels)
            batch_masks.append(edge_masks)
            batch_sparsities.append(sparsity)
        
        # è®¡ç®—ä¸»ä»»åŠ¡æŸå¤±
        scores = torch.cat(batch_scores)
        labels = torch.cat(batch_labels)
        
        # ä½¿ç”¨BCE loss
        task_loss = F.binary_cross_entropy_with_logits(scores, labels)
        
        # è®¡ç®—å‡†ç¡®ç‡å’ŒF1åˆ†æ•°
        pred = (scores > 0.5).float()
        current_correct = (pred == labels).sum().item()
        current_total = len(scores)
        current_accuracy = current_correct / current_total if current_total > 0 else 0
        
        # è®¡ç®—F1åˆ†æ•°
        true_positives = ((pred == 1) & (labels == 1)).sum().item()
        pred_positives = (pred == 1).sum().item()
        actual_positives = (labels == 1).sum().item()
        
        precision = true_positives / pred_positives if pred_positives > 0 else 0
        recall = true_positives / actual_positives if actual_positives > 0 else 0
        current_f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # è®¡ç®—è‡ªé€‚åº”ç¨€ç–æ€§æŸå¤±
        all_masks = torch.cat(batch_masks)
        sparsity_loss, adaptive_weight = model.compute_adaptive_sparsity_loss(
            all_masks, current_f1, model.training_steps.item()
        )
        
        # æ›´æ–°ç¨€ç–æ§åˆ¶å™¨å†å²
        avg_sparsity = np.mean(batch_sparsities)
        current_temperature = model.mask_temperature.item()
        model.sparsity_controller.update_history(current_f1, avg_sparsity, task_loss.item(), current_temperature)
        
        # æ–°å¢ï¼šæ¸©åº¦è‡ªåŠ¨è°ƒæ•´æœºåˆ¶
        with torch.no_grad():
            target_temperature = model.sparsity_controller.compute_temperature_adjustment(
                avg_sparsity, current_temperature
            )
            
            # å¹³æ»‘è°ƒæ•´æ¸©åº¦ï¼Œé¿å…å‰§çƒˆå˜åŒ–
            adjustment_factor = 0.1  # è°ƒæ•´é€Ÿåº¦
            new_temperature = current_temperature + adjustment_factor * (target_temperature - current_temperature)
            
            # é™åˆ¶æ¸©åº¦åœ¨å®‰å…¨èŒƒå›´å†…
            new_temperature = max(model.temperature_bounds[0], min(model.temperature_bounds[1], new_temperature))
            
            # æ›´æ–°æ¸©åº¦å‚æ•°
            model.mask_temperature.data.fill_(new_temperature)
            
            # è®°å½•æ¸©åº¦å˜åŒ–
            if abs(new_temperature - current_temperature) > 0.01:
                print(f"ğŸŒ¡ï¸ æ¸©åº¦è°ƒæ•´: {current_temperature:.3f} â†’ {new_temperature:.3f} (ç¨€ç–åº¦: {avg_sparsity:.3f})")
            
            # æ›´æ–°ä¸Šæ¬¡ç¨€ç–åº¦è®°å½•
            model.last_sparsity.data.fill_(avg_sparsity)
        
        # è·å–è´Ÿä¾‹ç´¢å¼•
        negative_idxs = []
        for i, candidates in enumerate(candidate_idxs):
            # é€‰æ‹©éç­”æ¡ˆçš„å€™é€‰é¡¹ä½œä¸ºè´Ÿä¾‹
            neg_candidates = [c for c in candidates if c != answer_idx[i].item()]
            if neg_candidates:
                negative_idxs.extend(neg_candidates)
        
        # åªæœ‰å½“æœ‰è¶³å¤Ÿçš„è´Ÿä¾‹æ—¶æ‰è®¡ç®—å¯¹æ¯”æŸå¤±
        if node_embeds is not None and len(negative_idxs) > 0:
            # ç¡®ä¿ç»´åº¦åŒ¹é…
            # å¦‚æœæˆ‘ä»¬åªæœ‰ä¸€ä¸ªbatchçš„åµŒå…¥ï¼Œä¼ é€’batchä¸­çš„ç¬¬ä¸€ä¸ªé—®é¢˜å’Œç­”æ¡ˆ
            # ä½¿ç”¨è´Ÿä¾‹åˆ›å»ºåˆé€‚ç»´åº¦çš„tensor
            try:
                contrast_loss = contrastive_loss(
                    node_embeds, 
                    question_idx[0] if question_idx.dim() > 0 else question_idx, 
                    answer_idx[0] if answer_idx.dim() > 0 else answer_idx,
                    torch.tensor(negative_idxs, device=device),
                    temperature=0.1
                )
                
                # åŠ å…¥æ€»æŸå¤± - ä½¿ç”¨åŠ¨æ€æƒé‡
                loss = task_loss + sparsity_loss + 0.1 * contrast_loss
            except Exception as e:
                print(f"è®¡ç®—å¯¹æ¯”æŸå¤±æ—¶å‡ºé”™: {e}")
                # è·³è¿‡å¯¹æ¯”æŸå¤±
                loss = task_loss + sparsity_loss
        else:
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„è´Ÿä¾‹ï¼Œåªä½¿ç”¨ä»»åŠ¡æŸå¤±å’Œç¨€ç–æ€§æŸå¤±
            loss = task_loss + sparsity_loss
        
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        loss.backward()
        optimizer.step()
        
        # æ›´æ–°è®­ç»ƒæ­¥æ•°
        with torch.no_grad():
            model.training_steps += 1.0
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        total_task_loss += task_loss.item()
        total_sparsity_loss += sparsity_loss.item()
        
        # è·Ÿè¸ªæ©ç ç¨€ç–ç‡
        for sparsity in batch_sparsities:
            all_sparsities.append(sparsity)
        
        correct += current_correct
        total += current_total
        
        # è·Ÿè¸ªF1åˆ†æ•°å’Œè‡ªé€‚åº”æƒé‡
        all_f1_scores.append(current_f1)
        # ä¿®å¤ï¼šä¸ºæ¯ä¸ªå›¾æ·»åŠ ç›¸åŒçš„è‡ªé€‚åº”æƒé‡ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…
        for _ in batch_sparsities:
            all_adaptive_weights.append(adaptive_weight)
        
        # è®°å½•åˆ°TensorBoard
        global_step = epoch * len(train_loader) + batch_idx
        writer.add_scalar('Train/Loss', loss.item(), global_step)
        writer.add_scalar('Train/TaskLoss', task_loss.item(), global_step)
        writer.add_scalar('Train/SparsityLoss', sparsity_loss.item(), global_step)
        writer.add_scalar('Train/SparsityRate', np.mean(batch_sparsities), global_step)
        writer.add_scalar('Train/Temperature', model.mask_temperature.item(), global_step)  # æ–°å¢æ¸©åº¦è®°å½•
        writer.add_scalar('Train/AdaptiveWeight', adaptive_weight, global_step)
        
        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
        progress_bar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'task_loss': f"{task_loss.item():.4f}",
            'sparsity_loss': f"{sparsity_loss.item():.4f}",
            'sparsity': f"{np.mean(batch_sparsities):.3f}",
            'temp': f"{model.mask_temperature.item():.2f}",  # æ·»åŠ æ¸©åº¦æ˜¾ç¤º
            'adaptive_w': f"{adaptive_weight:.5f}",
            'acc': f"{current_correct/current_total:.4f}" if current_total > 0 else "N/A"
        })
    
    # è®¡ç®—å¹³å‡å€¼
    avg_loss = total_loss / len(train_loader)
    avg_task_loss = total_task_loss / len(train_loader)
    avg_sparsity_loss = total_sparsity_loss / len(train_loader)
    avg_sparsity = np.mean(all_sparsities)
    avg_f1 = np.mean(all_f1_scores) if all_f1_scores else 0
    avg_adaptive_weight = np.mean(all_adaptive_weights) if all_adaptive_weights else 0
    accuracy = correct / total if total > 0 else 0
    
    # æ‰“å°epochç»“æœ
    print(f"Epoch {epoch}, Train Loss: {avg_loss:.4f}, Task Loss: {avg_task_loss:.4f}, "
          f"Sparsity Loss: {avg_sparsity_loss:.4f}, Acc: {accuracy:.4f}, F1: {avg_f1:.4f}, "
          f"Sparsity: {avg_sparsity:.4f}, Adaptive Weight: {avg_adaptive_weight:.5f}")
    
    # è®°å½•è¯¦ç»†æ•°æ®åˆ°TensorBoardï¼ˆç§»é™¤ç»˜å›¾ï¼Œåªè®°å½•æ•°æ®ï¼‰
    writer.add_scalar('Train/EpochLoss', avg_loss, epoch)
    writer.add_scalar('Train/EpochAccuracy', accuracy, epoch)
    writer.add_scalar('Train/EpochF1', avg_f1, epoch)
    writer.add_scalar('Train/EpochSparsityRate', avg_sparsity, epoch)
    writer.add_scalar('Train/EpochAdaptiveWeight', avg_adaptive_weight, epoch)
    
    # è®°å½•åˆ†å¸ƒæ•°æ®ï¼ˆç”¨äºåç»­å¯è§†åŒ–ï¼‰
    writer.add_histogram('Train/SparsityDistribution', torch.tensor(all_sparsities), epoch)
    writer.add_histogram('Train/AdaptiveWeightDistribution', torch.tensor(all_adaptive_weights), epoch)
    writer.add_histogram('Train/F1Distribution', torch.tensor(all_f1_scores), epoch)
    
    return avg_loss, accuracy, avg_f1, avg_sparsity

def validate(model, val_loader, device, epoch, writer):
    """éªŒè¯æ¨¡å‹å¹¶è¿”å›æŒ‡æ ‡"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    # è·Ÿè¸ªæ‰€æœ‰å›¾çš„æ©ç ç¨€ç–ç‡å’ŒF1åˆ†æ•°
    all_sparsities = []
    all_f1_scores = []
    
    with torch.no_grad():
        progress_bar = tqdm(val_loader, total=len(val_loader), desc="Validation", leave=False)
        for batch in progress_bar:
            g = batch['graph'].to(device)
            node_feats = batch['node_feats'].to(device)
            edge_weights = batch['edge_weights'].to(device)
            question_idx = batch['question_idx'].to(device)
            answer_idx = batch['answer_idx'].to(device)
            candidate_idxs = batch['candidate_idxs']
            
            # è®¡ç®—å€™é€‰ç­”æ¡ˆçš„åˆ†æ•°
            batch_scores = []
            batch_labels = []
            batch_sparsities = []
            
            # è·å–æ‰¹é‡å¤§å° - é¿å…ä½¿ç”¨len(g)
            batch_size = g.batch_size if hasattr(g, 'batch_size') else len(candidate_idxs)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚æ„å›¾
            is_hetero = hasattr(g, 'ntypes') and len(g.ntypes) > 1
            
            for i in range(batch_size):
                # è·å–å½“å‰å›¾çš„å€™é€‰ç­”æ¡ˆç´¢å¼•
                candidates = candidate_idxs[i]
                
                # è®¡ç®—æ¯ä¸ªå€™é€‰ç­”æ¡ˆçš„åˆ†æ•°å¹¶è·å–è¾¹æ©ç å’Œç¨€ç–ç‡
                if is_hetero:
                    # å¯¹äºå¼‚æ„å›¾ï¼Œæˆ‘ä»¬éœ€è¦å•ç‹¬å¤„ç†æ¯ä¸ªå›¾
                    current_g = g[i] if isinstance(g, list) else g
                    current_feats = node_feats[i]
                    current_weights = edge_weights[i] if edge_weights.dim() > 1 else edge_weights
                    current_q_idx = question_idx[i]
                else:
                    # å¯¹äºåŒæ„å›¾ï¼Œä½¿ç”¨æ‰¹å¤„ç†ç´¢å¼•
                    current_g = g
                    current_feats = node_feats
                    current_weights = edge_weights
                    current_q_idx = question_idx[i]
                
                scores, _, sparsity = model.compute_answer_scores(
                    current_g, current_feats, current_weights, 
                    current_q_idx, candidates
                )
                
                # åˆ›å»ºæ ‡ç­¾ï¼Œæ­£ç¡®çš„ç­”æ¡ˆæ ‡è®°ä¸º1ï¼Œå…¶ä»–æ ‡è®°ä¸º0
                labels = torch.zeros_like(scores)
                for j, c_idx in enumerate(candidates):
                    if c_idx == answer_idx[i].item():
                        labels[j] = 1
                        break
                
                batch_scores.append(scores)
                batch_labels.append(labels)
                batch_sparsities.append(sparsity)
            
            # æ‹¼æ¥ç»“æœ
            scores = torch.cat(batch_scores)
            labels = torch.cat(batch_labels)
            
            # è®¡ç®—æŸå¤±
            loss = F.binary_cross_entropy_with_logits(scores, labels)
            total_loss += loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            pred = (scores > 0.5).float()
            current_correct = (pred == labels).sum().item()
            correct += current_correct
            current_total = len(scores)
            total += current_total
            
            # è®¡ç®—F1åˆ†æ•°
            true_positives = ((pred == 1) & (labels == 1)).sum().item()
            pred_positives = (pred == 1).sum().item()
            actual_positives = (labels == 1).sum().item()
            
            precision = true_positives / pred_positives if pred_positives > 0 else 0
            recall = true_positives / actual_positives if actual_positives > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            # è·Ÿè¸ªç¨€ç–ç‡å’ŒF1åˆ†æ•°
            current_sparsity = np.mean(batch_sparsities)
            all_sparsities.extend(batch_sparsities)
            all_f1_scores.append(f1)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}", 
                'acc': f"{current_correct/current_total:.4f}",
                'f1': f"{f1:.4f}", 
                'sparsity': f"{current_sparsity:.3f}"
            })
    
    # è®¡ç®—å¹³å‡å€¼
    avg_loss = total_loss / len(val_loader)
    avg_sparsity = np.mean(all_sparsities)
    avg_f1 = np.mean(all_f1_scores) if all_f1_scores else 0
    accuracy = correct / total if total > 0 else 0
    
    # æ‰“å°ç»“æœ
    print(f"Validation: Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, F1: {avg_f1:.4f}, Sparsity: {avg_sparsity:.4f}")
    
    # è®°å½•åˆ°TensorBoardï¼ˆç§»é™¤ç»˜å›¾ï¼Œåªè®°å½•æ•°æ®ï¼‰
    writer.add_scalar('Validation/Loss', avg_loss, epoch)
    writer.add_scalar('Validation/Accuracy', accuracy, epoch)
    writer.add_scalar('Validation/F1', avg_f1, epoch)
    writer.add_scalar('Validation/SparsityRate', avg_sparsity, epoch)
    
    return avg_loss, accuracy, avg_f1, avg_sparsity

def analyze_edge_importance(model, graph_data, device):
    """åˆ†æè¾¹é‡è¦æ€§å¹¶å¯è§†åŒ–çƒ­å›¾"""
    model.eval()
    g = graph_data['graph'].to(device)
    node_feats = graph_data['node_feats'].to(device)
    edge_weights = graph_data['edge_weights'].to(device)
    
    with torch.no_grad():
        # å‰å‘ä¼ æ’­è·å–è¾¹æ©ç 
        _, edge_masks, sparsity = model.forward(g, node_feats, edge_weights)
        
        # è·å–è¾¹çš„æºå’Œç›®æ ‡èŠ‚ç‚¹
        edge_src, edge_dst = g.edges()
        
        # åˆ†æé‡è¦è¾¹
        important_edges = []
        for i in range(len(edge_src)):
            src, dst = edge_src[i], edge_dst[i]
            mask = edge_masks[i].item()
            
            if mask > 0.5:  # é‡è¦è¾¹é˜ˆå€¼
                important_edges.append((src.item(), dst.item(), mask))
        
        # æŒ‰é‡è¦æ€§æ’åº
        important_edges.sort(key=lambda x: x[2], reverse=True)
        
        # æ‰“å°æœ€é‡è¦çš„è¾¹
        print(f"æ©ç ç¨€ç–ç‡: {sparsity:.4f}, é‡è¦è¾¹æ•°é‡: {len(important_edges)}")
        for src, dst, mask in important_edges[:10]:
            print(f"è¾¹ ({src} -> {dst}): é‡è¦æ€§ = {mask:.4f}")
        
        return edge_masks, sparsity, important_edges

def check_early_stopping(sparsity_history, f1_history, patience=8):
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ - ä¿®æ”¹ä¸ºæ›´å®½æ¾çš„æ¡ä»¶
    
    æ¡ä»¶: å½“ç¨€ç–ç‡Ïâˆˆ[0.15, 0.50]ä¸”dev F1 8ä¸ªepochæ— æå‡æ—¶æ‰åœæ­¢
    """
    if len(sparsity_history) < patience or len(f1_history) < patience:
        return False
    
    # æ£€æŸ¥ç¨€ç–ç‡æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…ï¼ˆæ”¾å®½èŒƒå›´ï¼‰
    latest_sparsity = sparsity_history[-1]
    in_target_range = 0.05 <= latest_sparsity <= 0.60  # æ›´å®½çš„èŒƒå›´
    
    # æ£€æŸ¥F1æ˜¯å¦æœ‰æå‡ï¼ˆæ›´ä¸¥æ ¼çš„æ— æå‡æ¡ä»¶ï¼‰
    recent_f1 = list(f1_history)[-patience:]
    best_recent_f1 = max(recent_f1)
    no_improvement = best_recent_f1 <= f1_history[-patience] + 1e-4
    
    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœF1ä¸‹é™è¿‡å¤šï¼Œä¹Ÿè¦æ—©åœ
    recent_avg = np.mean(list(f1_history)[-3:])
    earlier_avg = np.mean(list(f1_history)[-6:-3]) if len(f1_history) >= 6 else recent_avg
    severe_drop = (earlier_avg - recent_avg) > 0.15  # F1ä¸‹é™è¶…è¿‡15%
    
    return in_target_range and no_improvement and not severe_drop

def main():
    parser = argparse.ArgumentParser(description='è½¯æ©ç GNNè®­ç»ƒ')
    parser.add_argument('--graph_dir', type=str, required=True, help='å›¾æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--hidden_dim', type=int, default=256, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=2, help='GNNå±‚æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--dropout', type=float, default=0.2, help='Dropoutç‡')
    parser.add_argument('--sparsity_weight', type=float, default=0.05, help='ç¨€ç–æ€§æŸå¤±æƒé‡(å·²å¼ƒç”¨ï¼Œä½¿ç”¨è‡ªé€‚åº”æƒé‡)')
    parser.add_argument('--sparsity_target', type=float, default=0.25, help='ç›®æ ‡ç¨€ç–ç‡ (0.1-0.4)')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--cuda', action='store_true', help='ä½¿ç”¨CUDA')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    print(f"[Route1] åˆ›å»ºè¾“å‡ºç›®å½•: {args.output_dir}")
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"[Route1] è¾“å‡ºç›®å½•åˆ›å»ºçŠ¶æ€: {os.path.exists(args.output_dir)}")
    
    checkpoint_dir = os.path.join(args.output_dir, 'checkpoints')
    print(f"[Route1] åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    print(f"[Route1] æ£€æŸ¥ç‚¹ç›®å½•åˆ›å»ºçŠ¶æ€: {os.path.exists(checkpoint_dir)}")
    
    # è®¾ç½®TensorBoard
    log_dir = os.path.join(args.output_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=log_dir)
    
    # é€‰æ‹©è®¾å¤‡
    device = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®...")
    all_files = [f for f in os.listdir(args.graph_dir) if f.endswith('.json')]
    random.shuffle(all_files)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    split = int(0.8 * len(all_files))
    train_files = all_files[:split]
    val_files = all_files[split:]
    
    train_dataset = GraphDataset(args.graph_dir, train_files)
    val_dataset = GraphDataset(args.graph_dir, val_files)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=collate_fn)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = SoftMaskGNN(
        in_dim=768, 
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        dropout=args.dropout,
        sparsity_target=args.sparsity_target
    )
    model.to(device)
    
    # è®¾ç½®æ€»è®­ç»ƒæ­¥æ•°
    total_steps = args.epochs * len(train_loader)
    model.sparsity_controller.set_total_steps(total_steps)
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    
    # è®­ç»ƒå¾ªç¯
    print("=" * 60)
    print("ğŸš€ å¼€å§‹Route1 GNNè½¯æ©ç è®­ç»ƒ")
    print("=" * 60)
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ¨¡å‹å‚æ•°: hidden_dim={args.hidden_dim}, num_layers={args.num_layers}")
    print(f"   è®­ç»ƒå‚æ•°: batch_size={args.batch_size}, lr={args.lr}, epochs={args.epochs}")
    print(f"   ç¨€ç–åŒ–ç›®æ ‡: {args.sparsity_target:.2f}")
    print(f"   æ•°æ®é›†: è®­ç»ƒ{len(train_dataset)}ä¸ª, éªŒè¯{len(val_dataset)}ä¸ª")
    print(f"   è®¾å¤‡: {device}")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 60)
    
    best_val_f1 = 0
    patience_counter = 0
    
    # æ¸…ç†æ—§çš„metrics.jsonæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    metrics_path = os.path.join(args.output_dir, 'metrics.json')
    if os.path.exists(metrics_path):
        print(f"å‘ç°ç°æœ‰çš„metrics.jsonæ–‡ä»¶ï¼Œæ˜¯å¦æ¸…ç†ï¼Ÿ(å»ºè®®æ¸…ç†ä»¥é¿å…æ•°æ®æ··ä¹±)")
        print(f"æ–‡ä»¶è·¯å¾„: {metrics_path}")
        # åœ¨è‡ªåŠ¨åŒ–ç¯å¢ƒä¸­ï¼Œç›´æ¥æ¸…ç†æ—§æ–‡ä»¶
        try:
            os.remove(metrics_path)
            print("å·²æ¸…ç†æ—§çš„metrics.jsonæ–‡ä»¶")
        except Exception as e:
            print(f"æ¸…ç†æ–‡ä»¶å¤±è´¥: {e}")
    
    # å†å²æŒ‡æ ‡è®°å½•ç”¨äºæ—©åœ
    sparsity_history = deque(maxlen=10)
    f1_history = deque(maxlen=10)
    
    for epoch in range(args.epochs):
        # è®­ç»ƒ
        train_loss, train_acc, train_f1, train_sparsity = train(
            model, train_loader, optimizer, device, epoch, writer, 
            sparsity_weight=args.sparsity_weight
        )
        
        # éªŒè¯
        val_loss, val_acc, val_f1, val_sparsity = validate(model, val_loader, device, epoch, writer)
        
        # æ›´æ–°å†å²æŒ‡æ ‡
        sparsity_history.append(val_sparsity)
        f1_history.append(val_f1)
        
        print(f"Epoch {epoch}:")
        print(f"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, Sparsity: {train_sparsity:.4f}")
        print(f"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, Sparsity: {val_sparsity:.4f}")
        
        # æ£€æŸ¥ç¨€ç–åº¦æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        sparsity_status = "æ­£å¸¸"
        if val_sparsity > 0.8:
            sparsity_status = "è¿‡é«˜"
        elif val_sparsity < 0.1:
            sparsity_status = "è¿‡ä½"
        print(f"  ç¨€ç–åº¦çŠ¶æ€: {sparsity_status} (ç›®æ ‡: {args.sparsity_target:.2f})")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        print(f"\n[Route1] æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {checkpoint_dir}")
        print(f"[Route1] æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨: {os.path.exists(checkpoint_dir)}")

        # å…ˆå®šä¹‰è·¯å¾„ï¼Œå†ä½¿ç”¨
        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')
        print(f"[Route1] å…·ä½“æ–‡ä»¶è·¯å¾„: {checkpoint_path}")
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'val_f1': val_f1,
            'val_sparsity': val_sparsity,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'train_f1': train_f1,
            'train_sparsity': train_sparsity,
            'val_acc': val_acc,
            'sparsity_controller_state': {
                'performance_history': list(model.sparsity_controller.performance_history),
                'sparsity_history': list(model.sparsity_controller.sparsity_history),
                'best_performance': model.sparsity_controller.best_performance
            }
        }, checkpoint_path)
        
        print(f"[Route1] æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {os.path.exists(checkpoint_path)}")
        
        # ä¿®å¤ï¼šæ”¹è¿›è®­ç»ƒæŒ‡æ ‡ä¿å­˜é€»è¾‘ï¼Œé¿å…é‡å¤epoch
        metrics = {
            'epoch': epoch,
            'train_loss': float(train_loss),
            'train_acc': float(train_acc),
            'train_f1': float(train_f1),
            'train_sparsity': float(train_sparsity),
            'val_loss': float(val_loss),
            'val_acc': float(val_acc),
            'val_f1': float(val_f1),
            'val_sparsity': float(val_sparsity),
            'sparsity_status': sparsity_status
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒepochçš„æ•°æ®ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›¿æ¢
        epoch_exists = False
        if os.path.exists(metrics_path):
            with open(metrics_path, 'r') as f:
                try:
                    all_metrics = json.load(f)
                except json.JSONDecodeError:
                    all_metrics = []
        else:
            all_metrics = []
        
        for i, existing_metric in enumerate(all_metrics):
            if existing_metric.get('epoch') == epoch:
                all_metrics[i] = metrics  # æ›¿æ¢ç°æœ‰æ•°æ®
                epoch_exists = True
                break
        
        # å¦‚æœä¸å­˜åœ¨ç›¸åŒepochï¼Œåˆ™è¿½åŠ 
        if not epoch_exists:
            all_metrics.append(metrics)
        
        # æŒ‰epochæ’åºç¡®ä¿æ•°æ®æœ‰åº
        all_metrics.sort(key=lambda x: x.get('epoch', 0))
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open(metrics_path, 'w') as f:
            json.dump(all_metrics, f, indent=2)
        
        print(f"[Route1] æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path} (å…±{len(all_metrics)}ä¸ªepoch)")
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜ä¸ºbest_model.pt
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_path = os.path.join(args.output_dir, 'best_model.pt')
            torch.save(model.state_dict(), best_model_path)
            print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒF1: {val_f1:.4f}")
            patience_counter = 0
        else:
            patience_counter += 1
        
        # æ”¹è¿›çš„æ—©åœæ¡ä»¶
        should_stop = False
        
        # æ¡ä»¶1ï¼šç¨€ç–åº¦åœ¨åˆç†èŒƒå›´å†…ä¸”F1åˆ†æ•°è¿ç»­5è½®æ— æå‡
        if 0.15 <= val_sparsity <= 0.4 and patience_counter >= 5:
            print(f"è§¦å‘æ—©åœï¼šç¨€ç–åº¦åœ¨åˆç†èŒƒå›´å†…({val_sparsity:.3f})ä¸”F1åˆ†æ•°5è½®æ— æå‡")
            should_stop = True
        
        # æ¡ä»¶2ï¼šç¨€ç–åº¦è¿‡é«˜ä¸”æŒç»­3è½®ä»¥ä¸Š
        elif len(sparsity_history) >= 3 and all(s > 0.8 for s in list(sparsity_history)[-3:]):
            print(f"è§¦å‘æ—©åœï¼šç¨€ç–åº¦æŒç»­è¿‡é«˜(>0.8)")
            should_stop = True
        
        # æ¡ä»¶3ï¼šè¶…è¿‡15è½®æ— æ”¹è¿›
        elif patience_counter >= 15:
            print(f"è§¦å‘æ—©åœï¼šè¶…è¿‡15è½®æ— æ”¹è¿›")
            should_stop = True
        
        if should_stop:
            break
    
    # è®­ç»ƒç»“æŸåï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹å¹¶åˆ†æç»“æœ
    best_model = SoftMaskGNN(
        in_dim=768, 
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        dropout=args.dropout,
        sparsity_target=args.sparsity_target
    )
    best_model.load_state_dict(torch.load(os.path.join(args.output_dir, 'best_model.pt')))
    best_model.to(device)
    
    # åˆ†æå‡ ä¸ªæ ·æœ¬çš„è¾¹é‡è¦æ€§
    print("\n=== æœ€ä½³æ¨¡å‹åˆ†æ ===")
    for i in range(min(3, len(val_dataset))):
        print(f"\nåˆ†ææ ·æœ¬ {i}:")
        edge_masks, sparsity, important_edges = analyze_edge_importance(
            best_model, val_dataset[i], device
        )
    
    # å…³é—­TensorBoard writer
    writer.close()
    
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}")
    print(f"æœ€ç»ˆç¨€ç–åº¦çŠ¶æ€: {sparsity_status}")

if __name__ == '__main__':
    main() 